{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6480bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import pos_tag\n",
    "import ir_datasets\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import nltk.stem as stemmer\n",
    "from nltk.corpus import stopwords,words\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk.tokenize as nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a005f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pycountry\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "     -------------------------------------- 10.1/10.1 MB 687.6 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from pycountry) (65.6.3)\n",
      "Building wheels for collected packages: pycountry\n",
      "  Building wheel for pycountry (pyproject.toml): started\n",
      "  Building wheel for pycountry (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681896 sha256=30874f556437dcbe522bb7a80bcfc7c6bf1da72e71d0f9ccabcf18637dd123b6\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\62\\4a\\9c\\7a46699df9efb845aa116fae5e52d8690fc442fef6d32213f7\n",
      "Successfully built pycountry\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-22.3.5\n"
     ]
    }
   ],
   "source": [
    "pip install pycountry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73647212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       _id  \\\n",
      "0  c67482ba-2019-04-18T13:32:05Z-00000-000   \n",
      "1  c67482ba-2019-04-18T13:32:05Z-00001-000   \n",
      "2  c67482ba-2019-04-18T13:32:05Z-00002-000   \n",
      "3  c67482ba-2019-04-18T13:32:05Z-00003-000   \n",
      "4  4d3d4471-2019-04-18T11:45:01Z-00000-000   \n",
      "\n",
      "                                            title  \\\n",
      "0    Contraceptive Forms for High School Students   \n",
      "1    Contraceptive Forms for High School Students   \n",
      "2    Contraceptive Forms for High School Students   \n",
      "3    Contraceptive Forms for High School Students   \n",
      "4  Australia should be a more significant country   \n",
      "\n",
      "                                                text  \\\n",
      "0  My opponent forfeited every round. None of my ...   \n",
      "1  How do you propose the school will fund your p...   \n",
      "2  Schools have no compelling interest in providi...   \n",
      "3  As a senior at my school. My group and I are f...   \n",
      "4  The resolution used by Pro *assumes* that Aust...   \n",
      "\n",
      "                                            metadata  \n",
      "0  {'stance': 'CON', 'url': 'https://www.debate.o...  \n",
      "1  {'stance': 'CON', 'url': 'https://www.debate.o...  \n",
      "2  {'stance': 'CON', 'url': 'https://www.debate.o...  \n",
      "3  {'stance': 'PRO', 'url': 'https://www.debate.o...  \n",
      "4  {'stance': 'CON', 'url': 'https://www.debate.o...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provide the path to your JSONL file\n",
    "jsonl_file_path = r'C:\\Users\\User\\.ir_datasets\\beir\\webis-touche2020\\webis-touche2020\\corpus.jsonl'\n",
    "\n",
    "# Read the JSONL file into a DataFrame\n",
    "df = pd.read_json(jsonl_file_path, lines=True)\n",
    "\n",
    "# Now you can work with the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd496cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       _id  \\\n",
      "0  c67482ba-2019-04-18T13:32:05Z-00000-000   \n",
      "1  c67482ba-2019-04-18T13:32:05Z-00001-000   \n",
      "2  c67482ba-2019-04-18T13:32:05Z-00002-000   \n",
      "3  c67482ba-2019-04-18T13:32:05Z-00003-000   \n",
      "4  4d3d4471-2019-04-18T11:45:01Z-00000-000   \n",
      "\n",
      "                                            title  \\\n",
      "0    Contraceptive Forms for High School Students   \n",
      "1    Contraceptive Forms for High School Students   \n",
      "2    Contraceptive Forms for High School Students   \n",
      "3    Contraceptive Forms for High School Students   \n",
      "4  Australia should be a more significant country   \n",
      "\n",
      "                                                text  \\\n",
      "0  My opponent forfeited every round. None of my ...   \n",
      "1  How do you propose the school will fund your p...   \n",
      "2  Schools have no compelling interest in providi...   \n",
      "3  As a senior at my school. My group and I are f...   \n",
      "4  The resolution used by Pro *assumes* that Aust...   \n",
      "\n",
      "                                            metadata  \n",
      "0  {'stance': 'CON', 'url': 'https://www.debate.o...  \n",
      "1  {'stance': 'CON', 'url': 'https://www.debate.o...  \n",
      "2  {'stance': 'CON', 'url': 'https://www.debate.o...  \n",
      "3  {'stance': 'PRO', 'url': 'https://www.debate.o...  \n",
      "4  {'stance': 'CON', 'url': 'https://www.debate.o...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provide the path to your JSONL file\n",
    "jsonl_file_path = r'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/corpus.jsonl'\n",
    "\n",
    "# Read the JSONL file into a DataFrame\n",
    "dff = pd.read_json(jsonl_file_path, lines=True)\n",
    "\n",
    "# Now you can work with the DataFrame\n",
    "print(dff.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140da03",
   "metadata": {},
   "source": [
    "# ======== Preproccesing =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03b72862",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_title = {}\n",
    "data_text={}\n",
    "for i, row in dff.iterrows():\n",
    "    # Tokenize the title and abstract and remove stop words\n",
    "    title = row['title']\n",
    "    text = row['text']\n",
    "    \n",
    "    # Add the tokenized title and abstract to the dictionary\n",
    "    data_title[str(row['_id'])] = str(title) \n",
    "    data_text[str(row['_id'])]=str(text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c33a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_percont_text(text):\n",
    "    percont_text = text.replace('%', ' percent') # Remove punctuations\n",
    "    return percont_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1fb0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "def expand_country_names(text):\n",
    "    # Check if the input text is a string\n",
    "    if isinstance(text, str):\n",
    "        for country in pycountry.countries:\n",
    "            text = text.replace(country.alpha_2, country.name)\n",
    "            text = text.replace(country.alpha_3, country.name)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f1295cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuations\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe8ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb2a3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08bb7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e60cf05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "def remove_stopwords(text):\n",
    "    # tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # join the remaining words back into a string\n",
    "    text_without_stopwords = ' '.join(words)\n",
    "    \n",
    "    return text_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1baf01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(query)\n",
    "\n",
    "    # Correct the spelling\n",
    "    corrected_query = str(blob.correct())\n",
    "\n",
    "    return corrected_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bce7104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c67482ba-2019-04-18T13:32:05Z-00000-000: ['contracept', 'form', 'high', 'school', 'student', 'oppon', 'forfeit', 'everi', 'round', 'none', 'argument', 'answer', 'like', 'idea', 'win', 'default', 'tule', 'good', 'student', 'get', 'involv', 'address', 'big', 'issu', 'like', 'teen', 'pregnanc', 'need', 'abl', 'answer', 'argument', 'like', 'mine', 'simpli', 'prepar', 'abstin', 'onli', 'type', 'respons', 'also', 'awar', 'u', 'condom', 'may', 'sold', 'minor', 'ani', 'state', 'retail', 'say', 'illeg', 'sell', 'frankli', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "def tokenize_and_remove_stop_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "# Create a dictionary to store the tokenized title/text\n",
    "tokenized_dict = {}\n",
    "\n",
    "# Loop through each row of the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    # Tokenize the title and text and remove stop words\n",
    "    title_tokens = tokenize_and_remove_stop_words(row['title'])\n",
    "    abstract_tokens = tokenize_and_remove_stop_words(row['text'])\n",
    "    \n",
    "    # Add the tokenized title and text to the dictionary\n",
    "    tokenized_dict[str(row['_id'])] = title_tokens + abstract_tokens\n",
    "\n",
    "    \n",
    "# Print the tokenized dictionary\n",
    "# Print the first 10 key-value pairs in the tokenized dictionary\n",
    "for i, (key, value) in enumerate(tokenized_dict.items()):\n",
    "    if i < 1:\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51478e99",
   "metadata": {},
   "source": [
    "# ========  End Preproccesing =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df33cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382545\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the tokenized_dict from file\n",
    "with open(r'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/tokenized_dict.json', 'r') as f:\n",
    "    tokenized_dict_f = json.load(f)\n",
    "\n",
    "# Example usage\n",
    "print(len(tokenized_dict_f)) # Print the number of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def create_inverted_index(tokenized_dict):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for x, doc_content in tokenized_dict.items():\n",
    "        # Combine the \"title\" and \"abstract\" fields\n",
    "        text = \" \".join(doc_content)\n",
    "        \n",
    "        # Tokenize the text and apply any desired text preprocessing steps\n",
    "        terms = [term.lower() for term in nltk.word_tokenize(text) if term.isalnum()]\n",
    "\n",
    "        # Add the doc_id to the list associated with each term in the inverted index\n",
    "        for term in terms:\n",
    "            inverted_index[term].append(x)\n",
    "    \n",
    "    return dict(inverted_index)\n",
    "\n",
    "# Assuming that the tokenized_dict is a dictionary with document ids as keys and tokenized titles/abstracts as values\n",
    "inverted_index = create_inverted_index(tokenized_dict)\n",
    "# Create a file object in write mode\n",
    "\n",
    "    \n",
    "\n",
    "for i, (key, value) in enumerate(inverted_index.items()):\n",
    "    if i < 10:\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14baa1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/inverted_index.json', 'w') as f:\n",
    "        json.dump(dict(inverted_index), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the tokenized_dict from file\n",
    "with open('C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/inverted_index.json', 'r') as f:\n",
    "    inverted_index_f = json.load(f)\n",
    "\n",
    "# Example usage\n",
    "print(len(inverted_index_f)) # Print the number of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a8bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "326edc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus():\n",
    "    i=1\n",
    "    temp={}\n",
    "    for key,value in tokenized_dict_f.items():\n",
    "        my_string = \" \".join(value)\n",
    "        temp[key]=my_string\n",
    "       \n",
    "    return temp\n",
    "\n",
    "temp=convert_corpus();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "318a06da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c67482ba-2019-04-18T13:32:05Z-00000-000\n",
      "contracept form high school student oppon forfeit everi round none argument answer like idea win default tule good student get involv address big issu like teen pregnanc need abl answer argument like mine simpli prepar abstin onli type respons also awar u condom may sold minor ani state retail say illeg sell frankli wrong\n"
     ]
    }
   ],
   "source": [
    "first_key = next(iter(temp))\n",
    "first_value = temp[first_key]\n",
    "print(first_key)\n",
    "print(first_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df1306",
   "metadata": {},
   "source": [
    "# ============ Calculate Tf_Idf And Read From file ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5618e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = list(map(str, temp.values()))  # Convert values of temp to strings using map\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eea093",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/User/.ir_datasets/beir/webis-touche2020/webis-touche2020/tfidf_matrix.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save the TF-IDF matrix to the specified file path\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m save_npz(file_path, \u001b[43mtfidf_matrix\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "documents = list(map(str, temp.values()))  # Convert values of temp to strings using map\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Specify the file path for storing the TF-IDF matrix\n",
    "file_path = 'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/tfidf_matrix.npz'\n",
    "\n",
    "# Save the TF-IDF matrix to the specified file path\n",
    "save_npz(file_path, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164d2ff",
   "metadata": {},
   "source": [
    "# ====== End calculate Tf_Idf ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7831ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c67482ba-2019-04-18T13:32:05Z-00000-000\n",
      "c67482ba-2019-04-18T13:32:05Z-00001-000\n",
      "c67482ba-2019-04-18T13:32:05Z-00002-000\n",
      "c67482ba-2019-04-18T13:32:05Z-00003-000\n",
      "4d3d4471-2019-04-18T11:45:01Z-00000-000\n",
      "contracept form high school student oppon forfeit everi round none argument answer like idea win default tule good student get involv address big issu like teen pregnanc need abl answer argument like mine simpli prepar abstin onli type respons also awar u condom may sold minor ani state retail say illeg sell frankli wrong\n",
      "contracept form high school student propos school fund program condom cost money check opt list befor hand take time away staff member whenev could actual job opt option onli token parent author would easili subvert everyon school except hand student access free condom think student would simpli ask friend provid condom\n",
      "contracept form high school student school compel interest provid contracept student purpos school provid healthcar provid ani servic except insofar relat educ though contest individu district ought thi option choos educ feel adequ sexual educ necessari tinyurl com tinyurl com tinyurl com\n",
      "contracept form high school student senior school group focus teenag pregnanc determin high school district provid contracept form student safe sex thi focu u encourag sex teenag decid sex pleas safe addit parent agre thi want opt form child child receiv thi form\n",
      "australia signific countri resolut use pro assum australia alreadi signific countri howev actual realiti firstli clarifi signific mean state qualiti b consequ import respond directli pro argument first assert australia invent amaz thing like wifinland googl map polym bank note ultrasound scanner stainless steel brace mani thing invent come australia consid signific countri countri home univers use invent centuri would seem pro wa tri argu australia simpli deserv recognit case propos instead state signific becaus exampl pro themselv ha list fulli go instead affirm resolut pro ha negat insignific countri invent thing wifi googl map one invent list pro take issu though ultrasound thi wa invent australia first use thought austria countri europ technolog develop use ultrasound medicin began dure shortli world war variou centr around world work dr karl theodor dussik austria transmiss ultrasound investig brain provid first publish work medic ultrason although worker unit statesaudi arabia japan europ also cite pioneer work professor ian donald hi colleagu glasgow mid much facilit develop practic technolog applic http www bmu org\n"
     ]
    }
   ],
   "source": [
    "keys=list(map(str, temp.keys()))\n",
    "i=0\n",
    "# Print the map\n",
    "for key in keys:\n",
    "    if(i<5):\n",
    "        print(key)\n",
    "        i+=1\n",
    "i=0\n",
    "# Print the map\n",
    "for doc in documents:\n",
    "    if(i<5):\n",
    "        print(doc)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfb7b1",
   "metadata": {},
   "source": [
    "# ========== ==== Befor Apply Algorithms================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "24ce2b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: Does lowering the federal corporate income tax rate create jobs?\n",
      "Does lowering the federal corporate income tax rate create jobs?\n",
      "lower feder corpor incom tax rate creat job\n",
      "382545\n",
      "[ 30336 273875  58640 273872 180419 180421 300239  58248  58643  33375]\n",
      "Title: Corporate Tax Should be Lowered\n",
      "Title: Corporate taxes should be lowered\n",
      "Title: Cut the Corporate Tax Rate to 0%\n",
      "Title: Corporate taxes should be lowered\n",
      "Title: Corporate Tax Should be Abolished\n",
      "Title: Corporate Tax Should be Abolished\n",
      "Title: Corporate tax plan ideas for job growth.\n",
      "Title: Implement a Flat Tax w/ Standard Deduction\n",
      "Title: Cut the Corporate Tax Rate to 0%\n",
      "Title: the federal income tax should be abolished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = input(\"Enter your query: \")\n",
    "pureQuery = query\n",
    "\n",
    "predicted_documents = []\n",
    "# Apply the defined functions on the input query\n",
    "query = correct_spelling(query)\n",
    "print(query)\n",
    "query = replace_percont_text(query)\n",
    "query = expand_country_names(query)\n",
    "query = clean_text(query)\n",
    "query = remove_stopwords(query)\n",
    "query = lemmatize_text(query)\n",
    "query = stem_text(query)\n",
    "\n",
    "print(query)\n",
    "\n",
    "# Transform the query into a TF-IDF vector\n",
    "query_vector = vectorizer.transform([query])\n",
    "\n",
    "# Compute the cosine similarity between the query vector and the documents' TF-IDF vectors\n",
    "cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_document_index = cosine_similarities.argmax()\n",
    "\n",
    "# Get the similarity score of the most similar document\n",
    "similarity_score = cosine_similarities[0, most_similar_document_index]\n",
    "print(len(cosine_similarities[0]))\n",
    "# Get the indices of the top 10 most similar documents\n",
    "top_indices = cosine_similarities.argsort()[0][-10:][::-1]\n",
    "print(top_indices)\n",
    "predicted_documents += [keys[idx] for idx in top_indices]\n",
    "\n",
    "    # Print the most similar documents and their similarity scores\n",
    "for idx in top_indices:  # Change the limit as per your requirement\n",
    "    most_similar_document_key = list(temp.keys())[idx]\n",
    "    print(\"Title:\", data_title[str(most_similar_document_key)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca3167",
   "metadata": {},
   "source": [
    "# ========== ==== End ================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7939a4a",
   "metadata": {},
   "source": [
    "# ========== ==== After Apply Algorithms================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da4d780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Clustering\n",
    "num_clusters = 5  # Set the number of clusters as desired\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ce6d20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the cluster labels\n",
    "cluster_data = pd.DataFrame({'Cluster Label': cluster_labels})\n",
    "\n",
    "# Specify the file path for storing the CSV file\n",
    "file_path = 'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/cluster_labels.csv'\n",
    "\n",
    "# Save the cluster labels to the specified file path\n",
    "cluster_data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b57e96",
   "metadata": {},
   "source": [
    "# ============Read Cluster_Label =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1ff8bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path for the CSV file\n",
    "file_path = 'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/cluster_labels.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "cluster_data = pd.read_csv(file_path)\n",
    "\n",
    "# Access the cluster labels from the DataFrame\n",
    "cluster_labels = cluster_data['Cluster Label'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "679be8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Topic Detection\n",
    "num_topics = 3  # Set the number of topics as desired\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "topic_weights = lda.fit_transform(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "30adf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame to store the topic weights\n",
    "topic_data = pd.DataFrame(topic_weights)\n",
    "\n",
    "# Specify the file path for storing the CSV file\n",
    "file_path = 'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/topic_weights.csv'\n",
    "\n",
    "# Save the topic weights to the specified file path\n",
    "topic_data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6256ea0",
   "metadata": {},
   "source": [
    "# =============Read topic_weights ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a73aad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path for the CSV file\n",
    "file_path = 'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/topic_weights.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "topic_data = pd.read_csv(file_path)\n",
    "\n",
    "# Access the topic weights from the DataFrame\n",
    "topic_weights = topic_data.values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e572a7d",
   "metadata": {},
   "source": [
    "# =========== Cluster =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "45cc55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: Does lowering the federal corporate income tax rate create jobs?\n",
      "The result of Cluster Algorithm search :  104998\n",
      "Title: Corporate Tax Should be Lowered\n",
      "Title: Corporate taxes should be lowered\n",
      "Title: Corporate Tax Should be Abolished\n",
      "Title: Implement a Flat Tax w/ Standard Deduction\n",
      "Title: Cut the Corporate Tax Rate to 0%\n",
      "Title: Taxes on the Rich should be Increased\n",
      "Title: Corporate Tax Should be Lowered\n",
      "Title: Resolved: The USFG should adopt across-the-board tax cuts for individual and corporate tax brackets\n",
      "Title: Corporate Tax Should be Lowered\n",
      "Title: Replacing the Current Tax Code With A National Sales Tax Will Reboot the American Economy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Query processing\n",
    "query = input(\"Enter your query: \")\n",
    "pureQuery = query\n",
    "\n",
    "# Apply the defined functions on the input query\n",
    "query = correct_spelling(query)\n",
    "query = replace_percont_text(query)\n",
    "query = expand_country_names(query)\n",
    "query = clean_text(query)\n",
    "query = remove_stopwords(query)\n",
    "query = lemmatize_text(query)\n",
    "query = stem_text(query)\n",
    "\n",
    "# Transform the query into a TF-IDF vector\n",
    "query_vector = vectorizer.transform([query])\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "most_similar_document_index = cosine_similarities.argmax()\n",
    "similarity_score = cosine_similarities[0, most_similar_document_index]\n",
    "\n",
    "# Cluster Algorithm\n",
    "query_cluster = cluster_labels[most_similar_document_index]\n",
    "cluster_indices = [i for i, label in enumerate(cluster_labels) if label == query_cluster]\n",
    "print(\"The result of Cluster Algorithm search : \",len(cluster_indices))\n",
    "cluster_similarities = cosine_similarities[0, cluster_indices]\n",
    "sorted_indices = np.array(cluster_indices)[np.argsort(cluster_similarities)[::-1]]\n",
    "predicted_documents += [keys[idx] for idx in sorted_indices]\n",
    "\n",
    "top_indices = sorted_indices[:10]\n",
    "\n",
    "# Print the most similar documents and their similarity scores\n",
    "for idx in top_indices:\n",
    "    most_similar_document_key = list(temp.keys())[idx]\n",
    "    print(\"Title:\", data_title[str(most_similar_document_key)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde8701",
   "metadata": {},
   "source": [
    "# ============ End Clustr ============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ed052",
   "metadata": {},
   "source": [
    "# =========== Topic ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f614ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: violence during pandemic\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'correct_spelling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m pureQuery \u001b[38;5;241m=\u001b[39m query\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Apply the defined functions on the input query\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_spelling\u001b[49m(query)\n\u001b[0;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m replace_percont_text(query)\n\u001b[0;32m     15\u001b[0m query \u001b[38;5;241m=\u001b[39m expand_country_names(query)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correct_spelling' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Query processing\n",
    "query = input(\"Enter your query: \")\n",
    "pureQuery = query\n",
    "\n",
    "# Apply the defined functions on the input query\n",
    "query = correct_spelling(query)\n",
    "query = replace_percont_text(query)\n",
    "query = expand_country_names(query)\n",
    "query = clean_text(query)\n",
    "query = remove_stopwords(query)\n",
    "query = lemmatize_text(query)\n",
    "query = stem_text(query)\n",
    "\n",
    "# Transform the query into a TF-IDF vector\n",
    "query_vector = vectorizer.transform([query])\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "most_similar_document_index = cosine_similarities.argmax()\n",
    "similarity_score = cosine_similarities[0, most_similar_document_index]\n",
    "\n",
    "# Topic Algorithm\n",
    "most_similar_topic_weights = topic_weights[most_similar_document_index]\n",
    "best_topic_index = most_similar_topic_weights.argmax()\n",
    "documents_in_best_topic = [i for i, weight in enumerate(topic_weights[:, best_topic_index]) if weight > 0]\n",
    "topic_similarities = cosine_similarities[0, documents_in_best_topic]\n",
    "sorted_indices_within_topic = np.array(documents_in_best_topic)[np.argsort(topic_similarities)[::-1]]\n",
    "\n",
    "top_indices_within_topic = sorted_indices_within_topic[:10]\n",
    "print(\"The result of Topic Algorithm search : \",len(top_indices_within_topic))\n",
    "predicted_documents += [keys[idx] for idx in sorted_indices]\n",
    "# Print the most similar documents within the best topic and their similarity scores\n",
    "for idx in top_indices_within_topic:\n",
    "    most_similar_document_key = list(temp.keys())[idx]\n",
    "    print(\"Title:\", data_title[str(most_similar_document_key)])\n",
    "\n",
    "# Print the best-weighted topic and its weight\n",
    "print(\"Best-weighted topic:\")\n",
    "print(\"Topic\", best_topic_index, \"- Weight:\", most_similar_topic_weights[best_topic_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5180647",
   "metadata": {},
   "source": [
    "# ================ End Topic =============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc316c",
   "metadata": {},
   "source": [
    "# =============== Evaluation ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94e1913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_id_from_text(text):\n",
    "    with open(r'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/queries.jsonl', 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data['text'] == text:\n",
    "                return data['_id']\n",
    "    return None\n",
    "\n",
    "\n",
    "query_number = get_id_from_text(pureQuery)\n",
    "print(query_number)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebebdbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query-id', 'corpus-id', 'score']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query_parts)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(query_parts[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mquery_number\u001b[49m):\n\u001b[0;32m      9\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_number' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open(r'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/qrels/test.tsv', 'r') as file:\n",
    "    for line in file:\n",
    "        query_parts = line.strip().split('\\t')\n",
    "        if i == 0:\n",
    "            print(query_parts)\n",
    "        try:\n",
    "            if int(query_parts[0]) == int(query_number):\n",
    "                data.append(i)\n",
    "        except ValueError:\n",
    "            pass  # Skip non-integer lines\n",
    "        i += 1\n",
    "\n",
    "print(data[0])\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "43a6daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "Precision@10: 0.6\n",
      "Precision: 0.6\n",
      "Recall: 0.003989361702127659\n",
      "MAP: 0.751388888888889\n",
      "MRR: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Read the relevance judgments from the file\n",
    "relevant_documents = []  # List of relevant document IDs for the query\n",
    "\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    tp = len(set(actual) & set(predicted))\n",
    "    print(len(predicted))\n",
    "    precision = tp / len(predicted)\n",
    "    return precision\n",
    "\n",
    "\n",
    "with open(r'C:/Users/barhom/.ir_datasets/beir/webis-touche2020/webis-touche2020/qrels/test.tsv', 'r') as file:\n",
    "    i = 0   \n",
    "    for line in file:\n",
    "        query_id, corpus_id, score = line.split()\n",
    "        if i >= data[0] and i <= data[-1]:\n",
    "            relevant_documents.append(corpus_id)     \n",
    "        i += 1   \n",
    "\n",
    "\n",
    "# Calculate Precision@10, Precision, and Recall\n",
    "precision_10 = precision_at_k(relevant_documents,predicted_documents, 10)\n",
    "precision = precision_at_k(relevant_documents, predicted_documents, len(predicted_documents))\n",
    "recall = len(set(relevant_documents) & set(predicted_documents)) / len(relevant_documents)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Mean Average Precision (MAP)\n",
    "average_precision = 0\n",
    "relevant_count = 0\n",
    "for i, doc in enumerate(predicted_documents):\n",
    "    if doc in relevant_documents:\n",
    "        relevant_count += 1\n",
    "        precision_at_i = relevant_count / (i + 1)\n",
    "        average_precision += precision_at_i\n",
    "if relevant_count > 0:\n",
    "    average_precision /= relevant_count\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Mean Reciprocal Rank (MRR)\n",
    "reciprocal_rank = 0\n",
    "for i, doc in enumerate(predicted_documents):\n",
    "    if doc in relevant_documents:\n",
    "        reciprocal_rank = 1 / (i + 1)\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Precision@10:\", precision_10)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"MAP:\", average_precision)\n",
    "print(\"MRR:\", reciprocal_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b61ab",
   "metadata": {},
   "source": [
    "# ============== End Evaluation ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b1239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
